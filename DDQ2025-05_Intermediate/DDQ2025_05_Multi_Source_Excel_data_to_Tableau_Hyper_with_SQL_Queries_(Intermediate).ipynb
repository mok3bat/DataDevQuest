{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3HGiee7PB3V9wo6THwFhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mok3bat/DataDevQuest/blob/main/DDQ2025_05_Multi_Source_Excel_data_to_Tableau_Hyper_with_SQL_Queries_(Intermediate).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSyTD3HKe8f_",
        "outputId": "c524acb7-0326-49a6-f590-e7e9fa91a3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pantab\n",
            "  Downloading pantab-5.2.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting tableauhyperapi\n",
            "  Downloading tableauhyperapi-0.0.22106-py3-none-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from pantab) (18.1.0)\n",
            "Requirement already satisfied: cffi!=1.14.3,<2,>=1.12.2 in /usr/local/lib/python3.11/dist-packages (from tableauhyperapi) (1.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi!=1.14.3,<2,>=1.12.2->tableauhyperapi) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Downloading pantab-5.2.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (77.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tableauhyperapi-0.0.22106-py3-none-manylinux2014_x86_64.whl (78.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pantab, jedi, tableauhyperapi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.2 pantab-5.2.2 tableauhyperapi-0.0.22106\n"
          ]
        }
      ],
      "source": [
        "# Install Libraries\n",
        "! pip install pandas pantab tableauhyperapi ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Libraries\n",
        "\n",
        "'''\n",
        "The selected code cell imports several Python libraries and modules that are commonly used for data manipulation, particularly when working with large datasets and Tableau Hyper files.\n",
        "\n",
        "Here's a breakdown of the imports:\n",
        "\n",
        "itertools: This module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML. It provides tools for working with iterators efficiently.\n",
        "time: This module provides various time-related functions, often used for measuring the execution time of code.\n",
        "numpy as np: Imports the NumPy library, aliasing it as np. NumPy is fundamental for numerical operations in Python, especially for working with arrays and matrices.\n",
        "pandas as pd: Imports the pandas library, aliasing it as pd. Pandas is a powerful library for data manipulation and analysis, providing data structures like DataFrames.\n",
        "pantab as pt: Imports the pantab library, aliasing it as pt. Pantab is a library that allows you to read and write pandas DataFrames to and from Tableau Hyper files.\n",
        "from tableauhyperapi import ...: This line imports specific components from the tableauhyperapi library. This library is the official Python API for working directly with Tableau Hyper files. The imported components are:\n",
        "Connection: Used to establish a connection to a Hyper file.\n",
        "CreateMode: Defines how to handle the creation of a Hyper file (e.g., create if it doesn't exist, replace if it does).\n",
        "HyperProcess: Manages the Hyper process, which is the engine that interacts with Hyper files.\n",
        "Inserter: Used to insert data into a table within a Hyper file.\n",
        "SqlType: Represents SQL data types used in Hyper tables.\n",
        "TableDefinition: Defines the schema (table name, column names, data types) of a table in a Hyper file.\n",
        "TableName: Represents the name of a table in a Hyper file.\n",
        "Telemetry: Used to configure telemetry settings for the Hyper process.\n",
        "'''\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pantab as pt\n",
        "from tableauhyperapi import (\n",
        "    Connection,\n",
        "    CreateMode,\n",
        "    HyperProcess,\n",
        "    Inserter,\n",
        "    SqlType,\n",
        "    TableDefinition,\n",
        "    TableName,\n",
        "    Telemetry,\n",
        "    escape_name,\n",
        "    escape_string_literal\n",
        ")\n",
        "\n",
        "from tabulate import tabulate\n",
        "import datetime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "564p-ciwgTi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccae052f-b5d2-4ff9-ec63-a6d8f9b8a480"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 1.46 ms (started: 2025-06-10 10:59:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "excel_path_1 = \"https://github.com/nikdutra/DDQ-2025-04/raw/refs/heads/main/datasets/RWFD_Supply_Chain.xlsx\"\n",
        "sheet_name_1 = \"OrderList\"\n",
        "\n",
        "df_orders = pd.read_excel(excel_path_1, sheet_name=sheet_name_1)\n",
        "\n",
        "print(tabulate(df_orders.head(), headers='keys', tablefmt='psql'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu27jB1QkDtz",
        "outputId": "a3e16d1e-d097-4649-ffa4-bc9be822490a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n",
            "|    |    Order ID | Order Date          | Origin Port   | Carrier   |   TPT | Service Level   |   Ship ahead day count |   Ship Late Day count | Customer   |   Product ID | Plant Code   | Destination Port   |   Unit quantity |   Weight |\n",
            "|----+-------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------|\n",
            "|  0 | 1.4473e+09  | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |             808 |    14.3  |\n",
            "|  1 | 1.44716e+09 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            3188 |    87.94 |\n",
            "|  2 | 1.44714e+09 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2331 |    61.2  |\n",
            "|  3 | 1.44736e+09 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |             847 |    16.16 |\n",
            "|  4 | 1.44736e+09 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2163 |    52.34 |\n",
            "+----+-------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the inferred data types\n",
        "print(df_orders.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6bLDVk7GlLC",
        "outputId": "4ff72920-08b5-4d0d-f70f-cbe5605b43ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order ID                         int64\n",
            "Order Date              datetime64[ns]\n",
            "Origin Port                     object\n",
            "Carrier                         object\n",
            "TPT                              int64\n",
            "Service Level                   object\n",
            "Ship ahead day count             int64\n",
            "Ship Late Day count              int64\n",
            "Customer                        object\n",
            "Product ID                       int64\n",
            "Plant Code                      object\n",
            "Destination Port                object\n",
            "Unit quantity                    int64\n",
            "Weight                         float64\n",
            "dtype: object\n",
            "time: 14.2 ms (started: 2025-06-10 07:40:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All types looks fine for me. Except for Order ID, Product ID which should be Int**"
      ],
      "metadata": {
        "id": "X14V1HRvnerP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change column datatype\n",
        "convert_dict = {'Product ID': int, 'Order ID': int}\n",
        "\n",
        "df_orders = df_orders.astype(convert_dict)\n",
        "print(df_orders.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoEElVnYn8Os",
        "outputId": "0c193220-0470-4aa8-8949-fcc6e9047a62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order ID                         int64\n",
            "Order Date              datetime64[ns]\n",
            "Origin Port                     object\n",
            "Carrier                         object\n",
            "TPT                              int64\n",
            "Service Level                   object\n",
            "Ship ahead day count             int64\n",
            "Ship Late Day count              int64\n",
            "Customer                        object\n",
            "Product ID                       int64\n",
            "Plant Code                      object\n",
            "Destination Port                object\n",
            "Unit quantity                    int64\n",
            "Weight                         float64\n",
            "dtype: object\n",
            "time: 5.93 ms (started: 2025-06-10 07:40:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "excel_path_2 = \"https://github.com/nikdutra/DDQ-2025-04/raw/refs/heads/main/datasets/RWFD_Solar_Energy.xlsx\"\n",
        "sheet_name_2 = \"Actuals\"\n",
        "\n",
        "df_actuals = pd.read_excel(excel_path_2, sheet_name=sheet_name_2)\n",
        "\n",
        "print(tabulate(df_actuals.head(), headers='keys', tablefmt='psql'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9PcU_48GETo",
        "outputId": "0b534d62-037c-4670-bdfc-4eb606656370"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "|    | Date                | Reading   | Location Site Name                 |   Latitude |   Longitude | PV              | Capacity   |   Power(MW) |\n",
            "|----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------|\n",
            "|  0 | 2020-02-27 23:00:00 | Actual    | 23MWDistribution PV-83.04 - 40.25  |      40.25 |      -83.05 | Distribution PV | 23MW       |         0   |\n",
            "|  1 | 2020-09-08 03:00:00 | Actual    | 16MWDistribution PV-83.15 - 41.149 |      41.15 |      -83.15 | Distribution PV | 16MW       |         0   |\n",
            "|  2 | 2020-09-02 03:00:00 | Actual    | 23MWDistribution PV-84.15 - 39.450 |      39.45 |      -84.15 | Distribution PV | 23MW       |         0   |\n",
            "|  3 | 2020-10-10 15:00:00 | Actual    | 27MWDistribution PV-84.45 - 39.450 |      39.45 |      -84.45 | Distribution PV | 27MW       |        30.2 |\n",
            "|  4 | 2020-09-02 00:00:00 | Actual    | 15MWDistribution PV-82.25 - 40.850 |      40.85 |      -82.25 | Distribution PV | 15MW       |         0   |\n",
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "time: 18.1 s (started: 2025-06-10 10:50:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the inferred data types\n",
        "print(df_actuals.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIqXcXr3mvF3",
        "outputId": "e9a603c4-5399-41f9-f7ff-46b21e1daac7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date                  datetime64[ns]\n",
            "Reading                       object\n",
            "Location Site Name            object\n",
            "Latitude                     float64\n",
            "Longitude                    float64\n",
            "PV                            object\n",
            "Capacity                      object\n",
            "Power(MW)                    float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get some info about the imported dataset.\n",
        "\n",
        "df_orders.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4qkFKQZoKUu",
        "outputId": "82a0fed7-8423-427d-c5ad-bfa7373a5a93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 599 entries, 0 to 598\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count  Dtype         \n",
            "---  ------                --------------  -----         \n",
            " 0   Order ID              599 non-null    int64         \n",
            " 1   Order Date            599 non-null    datetime64[ns]\n",
            " 2   Origin Port           599 non-null    object        \n",
            " 3   Carrier               599 non-null    object        \n",
            " 4   TPT                   599 non-null    int64         \n",
            " 5   Service Level         599 non-null    object        \n",
            " 6   Ship ahead day count  599 non-null    int64         \n",
            " 7   Ship Late Day count   599 non-null    int64         \n",
            " 8   Customer              599 non-null    object        \n",
            " 9   Product ID            599 non-null    int64         \n",
            " 10  Plant Code            599 non-null    object        \n",
            " 11  Destination Port      599 non-null    object        \n",
            " 12  Unit quantity         599 non-null    int64         \n",
            " 13  Weight                599 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(1), int64(6), object(6)\n",
            "memory usage: 65.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get some info about the imported dataset.\n",
        "\n",
        "df_actuals.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWVh1wyuHEa4",
        "outputId": "d3d47bf7-dd04-488d-fc2e-cb12f7a1e2b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 499 entries, 0 to 498\n",
            "Data columns (total 8 columns):\n",
            " #   Column              Non-Null Count  Dtype         \n",
            "---  ------              --------------  -----         \n",
            " 0   Date                499 non-null    datetime64[ns]\n",
            " 1   Reading             499 non-null    object        \n",
            " 2   Location Site Name  499 non-null    object        \n",
            " 3   Latitude            499 non-null    float64       \n",
            " 4   Longitude           499 non-null    float64       \n",
            " 5   PV                  499 non-null    object        \n",
            " 6   Capacity            499 non-null    object        \n",
            " 7   Power(MW)           499 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(3), object(4)\n",
            "memory usage: 31.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a datframe as a hyper file\n",
        "\n",
        "'''\n",
        "The function accept dataframe as input and write it to a hyper file.\n",
        "'''\n",
        "\n",
        "def write_via_pantab(df: pd.DataFrame, db_name=\"Extract_PanTab\", schema=\"Extract\", table_name=\"Extract\"):\n",
        "  # Let's write somewhere besides the default public schema\n",
        "  table = TableName(schema, table_name)\n",
        "  pt.frame_to_hyper(df, db_name+\".hyper\", table=table)"
      ],
      "metadata": {
        "id": "RfzXox-qh20L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54380b6-ae8e-4cfa-e062-606706bc4011"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 565 µs (started: 2025-06-10 11:42:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📚 Mapping Pandas dtypes → Tableau Hyper SqlType"
      ],
      "metadata": {
        "id": "cv0Tf93uwJh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping from pandas dtypes to Tableau SqlTypes\n",
        "pandas_to_hyper_type = {\n",
        "    \"int64\": SqlType.int(),            # Integer\n",
        "    \"int32\": SqlType.int(),\n",
        "    \"float64\": SqlType.double(),       # Float\n",
        "    \"float32\": SqlType.double(),\n",
        "    \"bool\": SqlType.bool(),            # Boolean\n",
        "    \"object\": SqlType.text(),          # String/Text\n",
        "    \"string\": SqlType.text(),\n",
        "    \"datetime64[ns]\": SqlType.timestamp(),  # Datetime\n",
        "    \"timedelta[ns]\": SqlType.interval(),    # Optional\n",
        "    \"category\": SqlType.text(),        # Fallback to text\n",
        "    \"UInt8\": SqlType.int(),            # Unsigned fallback\n",
        "    \"Int8\": SqlType.int(),\n",
        "    \"Int16\": SqlType.int(),\n",
        "    \"UInt16\": SqlType.int(),\n",
        "    \"UInt32\": SqlType.int(),\n",
        "    \"UInt64\": SqlType.int()\n",
        "}"
      ],
      "metadata": {
        "id": "xBrMXd2Kqf1H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🛠️ Function to Generate Hyper Schema from DataFrame"
      ],
      "metadata": {
        "id": "n9PmP5IYwoqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_hyper_schema(df, table_name=\"Extract\"):\n",
        "    column_definitions = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        dtype_str = str(df[col].dtype)\n",
        "        hyper_type = pandas_to_hyper_type.get(dtype_str, SqlType.text())  # default to text\n",
        "        column=TableDefinition.Column(name=col, type=hyper_type)\n",
        "        column_definitions.append(column)\n",
        "\n",
        "    return TableDefinition(table_name=table_name, columns=column_definitions)"
      ],
      "metadata": {
        "id": "S3tJ5W38wnkF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Since pandas treats spatial data as object dtype (e.g., WKT strings), you'll need to:\n",
        "\n",
        "*   Decide based on column name convention (like geometry, wkt, etc.)\n",
        "*   Or inspect sample values to detect WKT strings (e.g., \"POINT(30 10)\", \"POLYGON((...))\")\n"
      ],
      "metadata": {
        "id": "0WPYnNH2ywll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Update the Mapping Function"
      ],
      "metadata": {
        "id": "IZKsJzEBzNlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function will handle spatial columns\n",
        "User will define which column is including spatial info.\n",
        "\n",
        "Example\n",
        "df = pd.DataFrame({\n",
        "    \"location\": [\"POINT(30 10)\", \"POINT(40 20)\", \"POINT(50 30)\"],\n",
        "    \"city\": [\"Cairo\", \"Dubai\", \"Riyadh\"]\n",
        "})\n",
        "\n",
        "schema = infer_hyper_schema_with_spatial(df, spatial_columns=[\"location\"])\n",
        "print(schema)\n",
        "\n",
        "'''\n",
        "\n",
        "def infer_hyper_schema_with_spatial(df, schema=\"Extract\", table_name=\"Extract\", spatial_columns=None):\n",
        "\n",
        "    spatial_columns = spatial_columns or []\n",
        "\n",
        "    column_definitions = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col in spatial_columns:\n",
        "            hyper_type = SqlType.geography()\n",
        "        else:\n",
        "            dtype_str = str(df[col].dtype)\n",
        "            hyper_type = pandas_to_hyper_type.get(dtype_str, SqlType.text())  # default to text\n",
        "\n",
        "        column=TableDefinition.Column(name=col, type=hyper_type)\n",
        "        column_definitions.append(column)\n",
        "\n",
        "\n",
        "    return TableDefinition(table_name=TableName(schema, table_name), columns=column_definitions)"
      ],
      "metadata": {
        "id": "d9I0yGYkyvf-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer_hyper_schema_with_spatial(df=df_actuals, schema=\"Extract\", table_name=\"Extract\", spatial_columns=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaWoAcUmY_52",
        "outputId": "caaf32e2-0825-4ec2-e9fc-7e630aed037b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TableDefinition(TableName('Extract', 'Extract'), [Column('Date', SqlType.timestamp(), Nullability.NULLABLE), Column('Reading', SqlType.text(), Nullability.NULLABLE), Column('Location Site Name', SqlType.text(), Nullability.NULLABLE), Column('Latitude', SqlType.double(), Nullability.NULLABLE), Column('Longitude', SqlType.double(), Nullability.NULLABLE), Column('PV', SqlType.text(), Nullability.NULLABLE), Column('Capacity', SqlType.text(), Nullability.NULLABLE), Column('Power(MW)', SqlType.double(), Nullability.NULLABLE)], Persistence.PERMANENT)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write rows using hyper api\n",
        "\n",
        "def write_via_hyperapi(df: pd.DataFrame, db_name=\"Extract_API\", schema=\"Extract\", table_name=\"Extract\", spatial_columns=None):\n",
        "    with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
        "        table = infer_hyper_schema_with_spatial(df, schema=schema, table_name=table_name, spatial_columns=spatial_columns)\n",
        "        with Connection(\n",
        "            endpoint=hyper.endpoint,\n",
        "            database=db_name+\".hyper\",\n",
        "            create_mode=CreateMode.CREATE_AND_REPLACE,\n",
        "        ) as conn:\n",
        "            conn.catalog.create_schema(schema=table.table_name.schema_name)\n",
        "            # print(table.table_name.schema_name)\n",
        "            conn.catalog.create_table(table_definition=table)\n",
        "            with Inserter(conn, table) as inserter:\n",
        "              inserter.add_rows(rows=df.to_numpy().tolist())\n",
        "              inserter.execute()"
      ],
      "metadata": {
        "id": "k3MG2Z46h9uX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_existing_hyper_file(path_to_database, schema, table_name, query):\n",
        "    \"\"\"\n",
        "    Query data from an Hyper file and Create a new table under same schema.\n",
        "    \"\"\"\n",
        "    print(\"Query data from an Hyper file and Create a new table under same schema\")\n",
        "\n",
        "    # Starts the Hyper Process with telemetry enabled to send data to Tableau.\n",
        "    # To opt out, simply set telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU.\n",
        "\n",
        "    with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
        "\n",
        "        # Connect to existing Hyper file.\n",
        "        with Connection(endpoint=hyper.endpoint,\n",
        "                        # Using Create Mode Here will raise error\n",
        "                        database=path_to_database) as connection:\n",
        "\n",
        "                        # The table names in the \"Extract\" schema (the default schema).\n",
        "                        table_names = connection.catalog.get_table_names(schema=schema)\n",
        "                        table_exists = False\n",
        "\n",
        "                        for table in table_names:\n",
        "                          # Confirm that table exists\n",
        "\n",
        "                          if str(table) == f'\"{schema}\".\"{table_name}\"':\n",
        "                            print(str(table))\n",
        "                            table_definition = connection.catalog.get_table_definition(name=table)\n",
        "                            #table_sl = connection.catalog.get_table_definition(name=table)\n",
        "                            #print(f\"Table {table.name} has qualified name: {table}\")\n",
        "                            table_exists = True\n",
        "\n",
        "                            # get Columns Names for creating a new sliced dataframe based on the provided query\n",
        "                            col=[]\n",
        "                            for column in table_definition.columns:\n",
        "                              #print(f\"Column {column.name} has type={column.type} and nullability={column.nullability}\")\n",
        "                              col.append(column.name.unescaped)\n",
        "\n",
        "                            # Print all rows from the \"Extract\".\"Extract\" table.\n",
        "                            table_name = TableName(schema, table_name)\n",
        "                            print(f\"Sample of quried ros in the table {table_name}: using API approach\")\n",
        "                            # `execute_list_query` executes a SQL query and returns the result as list of rows of data,\n",
        "                            # each represented by a list of objects.\n",
        "                            selected_rows = connection.execute_list_query(query=query)\n",
        "\n",
        "                            df = pd.DataFrame(selected_rows, columns=col)\n",
        "                            print(tabulate(df.head(), headers='keys', tablefmt='psql'))\n",
        "                            print('Sliced dataframe has shape:', df.shape)\n",
        "\n",
        "                            #use timestamp to be added to filename to make it unique\n",
        "                            #tried to define the creation mode in the connection but failed\n",
        "                            insertion_ts=f'{datetime.datetime.now():%Y-%m-%d %H:%M:%S}'\n",
        "\n",
        "                            table_sl_table_name = f\"{table_name.name.unescaped}_Sliced_{insertion_ts}\"\n",
        "                            table_sl_db_name=path_to_database.split('/')[-1].split('.')[0]\n",
        "\n",
        "                            #print(table_definition)\n",
        "                            #Create a table definition for the sliced data to be inserted in same db, schema\n",
        "                            table_sl=TableDefinition(table_name=TableName(table_sl_db_name, schema, table_sl_table_name), columns=table_definition.columns)\n",
        "                            #print(table_sl)\n",
        "\n",
        "                            connection.catalog.create_table(table_definition=table_sl)\n",
        "                            with Inserter(connection, table_sl) as inserter:\n",
        "                              inserter.add_rows(rows=selected_rows)\n",
        "                              inserter.execute()\n",
        "\n",
        "\n",
        "                        if not table_exists:\n",
        "                          print(f\"Table {table_name} does not exist\")\n",
        "\n",
        "\n",
        "        print(\"The connection to the Hyper file has been closed.\")\n",
        "    print(\"The Hyper process has been shut down.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjPyc7G7NA5b",
        "outputId": "23a77aae-0700-4dfa-a5bd-4e3c3b261449"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.98 ms (started: 2025-06-10 12:08:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWDRdDMNjtIQ",
        "outputId": "1e98bca0-9b8c-41e5-cbfc-376b92e86a14"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 475 µs (started: 2025-06-10 11:12:34 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Method 1 – The Pantab Approach\n",
        "\n",
        "**Now let’s implement our first method using pantab:**\n",
        "\n",
        "1.   Set up a path for your Hyper file\n",
        "2.   Time the process with `time.time()`\n",
        "3. Use `pantab.frames_to_hyper()` to create a Hyper file with both tables\n",
        "4. Write SQL queries to filter each table (Weight > 50 for OrderList, Latitude < 40 for Actuals)\n",
        "5. Use `pantab.frame_from_hyper_query()` to execute each SQL query\n",
        "6. Save the filtered results back to a Hyper file\n",
        "7. Calculate and print the processing times"
      ],
      "metadata": {
        "id": "pIZN1_rzKacF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pantab.frames_to_hyper() to create a Hyper file with both tables\n",
        "write_via_pantab(df=df_orders, db_name=\"SupplyChain_PanTab\", schema=\"Private\", table_name=\"Orders\")\n",
        "write_via_pantab(df=df_actuals, db_name=\"SolarEnergy_PanTab\", schema=\"Private\", table_name=\"Actuals\")\n",
        "\n",
        "#Write SQL queries to filter each table (Weight > 50 for OrderList, Latitude < 40 for Actuals)\n",
        "query_orders = \"\"\"\n",
        "SELECT *\n",
        "FROM \"Private\".\"Orders\"\n",
        "WHERE \"Weight\" > 50\n",
        "\"\"\"\n",
        "\n",
        "query_actuals = \"\"\"\n",
        "SELECT *\n",
        "FROM \"Private\".\"Actuals\"\n",
        "WHERE \"Latitude\" < 40\n",
        "\"\"\"\n",
        "\n",
        "# Read a subset of the data from the Hyper file\n",
        "# Use pantab.frame_from_hyper_query() to execute each SQL query\n",
        "df_orders_filtered = pt.frame_from_hyper_query('/content/SupplyChain_PanTab.hyper', query_orders)\n",
        "print('Sample records of queried data from SupplyChain dataset using PanTab')\n",
        "print(tabulate(df_orders_filtered.head(), headers='keys', tablefmt='psql'))\n",
        "print('Sliced dataframe has shape:', df_orders_filtered.shape)\n",
        "print('\\n')\n",
        "print('-'*50)\n",
        "\n",
        "df_actuals_filtered = pt.frame_from_hyper_query('/content/SolarEnergy_PanTab.hyper', query_actuals)\n",
        "print('Sample records of queried data from SolarEnergy dataset using PanTab')\n",
        "print(tabulate(df_actuals_filtered.head(), headers='keys', tablefmt='psql'))\n",
        "print('Sliced dataframe has shape:', df_actuals_filtered.shape)\n",
        "\n",
        "# Save the filtered results to a Hyper file\n",
        "# Tried to write the sliced dataframe to same db, schema. However it replaced the whole file.\n",
        "# This limitatio of pantab\n",
        "write_via_pantab(df=df_orders_filtered, db_name=\"SupplyChain_Sliced_PanTab\", schema=\"Private_new\", table_name=\"Orders_sliced\")\n",
        "write_via_pantab(df=df_actuals_filtered, db_name=\"SolarEnergy_Sliced_PanTab\", schema=\"Private_new\", table_name=\"Actuals_sliced\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBW5UiOFG4nc",
        "outputId": "f8bb724e-4238-4670-c3ad-6071713534cc"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample records of queried data from SupplyChain dataset using PanTab\n",
            "+----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n",
            "|    |   Order ID | Order Date          | Origin Port   | Carrier   |   TPT | Service Level   |   Ship ahead day count |   Ship Late Day count | Customer   |   Product ID | Plant Code   | Destination Port   |   Unit quantity |   Weight |\n",
            "|----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------|\n",
            "|  0 | 1447158014 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            3188 |    87.94 |\n",
            "|  1 | 1447138898 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2331 |    61.2  |\n",
            "|  2 | 1447363980 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2163 |    52.34 |\n",
            "|  3 | 1447351440 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            3332 |    92.8  |\n",
            "|  4 | 1447398415 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2294 |    62.2  |\n",
            "+----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n",
            "Sliced dataframe has shape: (104, 14)\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Sample records of queried data from SolarEnergy dataset using PanTab\n",
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "|    | Date                | Reading   | Location Site Name                 |   Latitude |   Longitude | PV              | Capacity   |   Power(MW) |\n",
            "|----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------|\n",
            "|  0 | 2020-09-02 03:00:00 | Actual    | 23MWDistribution PV-84.15 - 39.450 |      39.45 |      -84.15 | Distribution PV | 23MW       |         0   |\n",
            "|  1 | 2020-10-10 15:00:00 | Actual    | 27MWDistribution PV-84.45 - 39.450 |      39.45 |      -84.45 | Distribution PV | 27MW       |        30.2 |\n",
            "|  2 | 2020-08-31 19:00:00 | Actual    | 35MWDistribution PV-83.15 - 39.850 |      39.85 |      -83.15 | Distribution PV | 35MW       |         0   |\n",
            "|  3 | 2020-04-16 01:00:00 | Actual    | 35MWDistribution PV-83.04 - 39.850 |      39.85 |      -83.05 | Distribution PV | 35MW       |         0   |\n",
            "|  4 | 2020-12-16 09:00:00 | Actual    | Oak Hills Array                    |      39.15 |      -84.65 | Distribution PV | 32MW       |        94.6 |\n",
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "Sliced dataframe has shape: (177, 8)\n",
            "time: 570 ms (started: 2025-06-10 11:53:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Method 2 – The Hyper API Approach\n",
        "\n",
        "**Time for the more detailed approach:**\n",
        "\n",
        "1. Define a path for your second Hyper file\n",
        "2. Start timing with `time.time()`\n",
        "3. Use HyperProcess and Connection to create a Hyper database\n",
        "4. Create appropriate table schemas based on your DataFrame columns\n",
        "5. Insert data from both DataFrames into your tables\n",
        "6. Run SQL queries against your tables using the Hyper API\n",
        "7. Create a new Hyper file with the filtered results\n",
        "8. Calculate and print the processing times"
      ],
      "metadata": {
        "id": "lpWUEAANRjOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pantab.frames_to_hyper() to create a Hyper file with both tables\n",
        "\n",
        "write_via_hyperapi(df=df_orders, db_name=\"SupplyChain_HyperAPI\", schema=\"Private\", table_name=\"Orders\")\n",
        "write_via_hyperapi(df=df_actuals, db_name=\"SolarEnergy_HyperAPI\", schema=\"Private\", table_name=\"Actuals\")\n",
        "\n",
        "#Write SQL queries to filter each table (Weight > 50 for OrderList, Latitude < 40 for Actuals)\n",
        "query_orders = \"\"\"\n",
        "SELECT *\n",
        "FROM \"Private\".\"Orders\"\n",
        "WHERE \"Weight\" > 50\n",
        "\"\"\"\n",
        "\n",
        "query_actuals = \"\"\"\n",
        "SELECT *\n",
        "FROM \"Private\".\"Actuals\"\n",
        "WHERE \"Latitude\" < 40\n",
        "\"\"\"\n",
        "\n",
        "# Read a subset of the data from the Hyper file\n",
        "query_existing_hyper_file(path_to_database='/content/SupplyChain_HyperAPI.hyper', schema='Private', table_name='Orders', query=query_orders)\n",
        "query_existing_hyper_file(path_to_database='/content/SolarEnergy_HyperAPI.hyper', schema='Private', table_name='Actuals', query=query_actuals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJH76XmBSNeT",
        "outputId": "df7574d4-722a-4a59-c84e-2371cdf58bed"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query data from an Hyper file and Create a new table under same schema\n",
            "\"Private\".\"Orders\"\n",
            "Sample of quried ros in the table \"Private\".\"Orders\": using API approach\n",
            "+----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n",
            "|    |   Order ID | Order Date          | Origin Port   | Carrier   |   TPT | Service Level   |   Ship ahead day count |   Ship Late Day count | Customer   |   Product ID | Plant Code   | Destination Port   |   Unit quantity |   Weight |\n",
            "|----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------|\n",
            "|  0 | 1447158014 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            3188 |    87.94 |\n",
            "|  1 | 1447138898 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2331 |    61.2  |\n",
            "|  2 | 1447363980 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2163 |    52.34 |\n",
            "|  3 | 1447351440 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            3332 |    92.8  |\n",
            "|  4 | 1447398415 | 2013-05-26 00:00:00 | PORT09        | V44_3     |     1 | CRF             |                      3 |                     0 | V55555_53  |      1700106 | PLANT16      | PORT09             |            2294 |    62.2  |\n",
            "+----+------------+---------------------+---------------+-----------+-------+-----------------+------------------------+-----------------------+------------+--------------+--------------+--------------------+-----------------+----------+\n",
            "Sliced dataframe has shape: (104, 14)\n",
            "The connection to the Hyper file has been closed.\n",
            "The Hyper process has been shut down.\n",
            "Query data from an Hyper file and Create a new table under same schema\n",
            "\"Private\".\"Actuals\"\n",
            "Sample of quried ros in the table \"Private\".\"Actuals\": using API approach\n",
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "|    | Date                | Reading   | Location Site Name                 |   Latitude |   Longitude | PV              | Capacity   |   Power(MW) |\n",
            "|----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------|\n",
            "|  0 | 2020-09-02 03:00:00 | Actual    | 23MWDistribution PV-84.15 - 39.450 |      39.45 |      -84.15 | Distribution PV | 23MW       |         0   |\n",
            "|  1 | 2020-10-10 15:00:00 | Actual    | 27MWDistribution PV-84.45 - 39.450 |      39.45 |      -84.45 | Distribution PV | 27MW       |        30.2 |\n",
            "|  2 | 2020-08-31 19:00:00 | Actual    | 35MWDistribution PV-83.15 - 39.850 |      39.85 |      -83.15 | Distribution PV | 35MW       |         0   |\n",
            "|  3 | 2020-04-16 01:00:00 | Actual    | 35MWDistribution PV-83.04 - 39.850 |      39.85 |      -83.05 | Distribution PV | 35MW       |         0   |\n",
            "|  4 | 2020-12-16 09:00:00 | Actual    | Oak Hills Array                    |      39.15 |      -84.65 | Distribution PV | 32MW       |        94.6 |\n",
            "+----+---------------------+-----------+------------------------------------+------------+-------------+-----------------+------------+-------------+\n",
            "Sliced dataframe has shape: (177, 8)\n",
            "The connection to the Hyper file has been closed.\n",
            "The Hyper process has been shut down.\n",
            "time: 498 ms (started: 2025-06-10 12:09:36 +00:00)\n"
          ]
        }
      ]
    }
  ]
}